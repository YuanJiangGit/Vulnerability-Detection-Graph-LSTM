# -*- coding: utf-8 -*-
# @Author  : Jiang Yuan
# @Time    : 2021/12/7 19:42
# @Function:
from tree_sitter import Language, Parser
import pandas as pd
from tqdm import tqdm
from data_process.get_tokens import *
from data_process.mapping import mapping
import pickle
import json
import os
import nltk

class DataPipline:
    def __init__(self, ratio, data_type):
        self.ratio = ratio
        root = os.getcwd() + '/../resources/'
        self.root = os.path.join(root, data_type)
        self.data_type = data_type
        self.count=0
        self.source_code_path = '../resources/Program/Source_code/'


    def get_id_dict(self):
        ids = os.listdir(self.source_code_path)
        id_dict = {}
        for id in ids:
            id_dict[id] = id
        return id_dict

    def correct_cwe_ids(self):
        '''
        统计含有补丁的CWE ID，这些我们认为是被正确解析
        :return:
        '''
        id_dict = self.get_id_dict()
        real_id_dict = {key: value for key, value in id_dict.items() if key.startswith('CVE-')}
        correct_ids = []
        for id in real_id_dict:
            vul_info_path = os.path.join(self.source_code_path, id,'vul_info.txt')
            with open(vul_info_path,'r') as f:
                vul_info = json.load(f)
            if len(vul_info)>0:
                correct_ids.append(id)
        return correct_ids

    def gen_data(self, input_slice_dir):
        '''
        对每个数据实例，生成符合Tree-LSTM模型的输入形式
        '''
        columns = ['Testid', 'tree-graph', 'label', 'file_name']
        sources = pd.DataFrame(columns=columns)
        # load data
        slice_graph_names = ['api_graphs.txt', 'arraysuse_graphs.txt','integeroverflow_graphs.txt','pointersuse_graphs.txt']
        slice_label_names = ['api_slices_label.json', 'arraysuse_slices_label.json', 'integeroverflow_slices_label.json', 'pointersuse_slices_label.json']
        cwe_ids = self.correct_cwe_ids()
        # 构建临时保存数据的文件夹
        if not os.path.exists(os.path.join(self.root,'temp_data')):
            os.mkdir(os.path.join(self.root,'temp_data'))
        for slice_graph, slice_label in zip(slice_graph_names, slice_label_names):
            slice_data_path = os.path.join(input_slice_dir, slice_graph)
            slice_label_path = os.path.join(input_slice_dir, slice_label)

            with open(slice_label_path,'r') as f:
                slice_label_dict = json.load(f)

            with open(slice_data_path,'r') as f:
                file_content = f.read()
            store_pkl_count = 0
            inst_list = file_content.split('------------------------------\n')
            for index, sg_content in tqdm(enumerate(inst_list)):
                d_dict = {}
                sg_state_list = sg_content.split('\n')
                if len(sg_state_list)<2:
                    continue
                sg_gen_info = sg_state_list[0]
                sg = json.loads(sg_state_list[1])

                vul_file_name = sg_gen_info.split('/')[-1]
                testid = sg_gen_info.split('/')[-2]

                # 过滤掉不合法的cwe id
                if testid.startswith('CVE-') and testid not in cwe_ids:
                    continue

                # if testid.startswith('CVE-'):
                #     print('cve')

                label = slice_label_dict[sg_gen_info]
                label = label.replace('\n', '')
                # 获取粗粒度标签
                if label =='[0]':
                    coarse_label = 0
                else:
                    coarse_label = 1
                # （处理特殊情况） CVE的第一行（漏洞语句）可能是函数名，只有函数名变化不是漏洞
                if coarse_label == 1 and 'CVE-' in testid:
                    if label == '[1]' or label == '[2]':
                        coarse_label = 0

                d_dict = {'Testid':testid, 'tree-graph':sg, 'label':coarse_label, 'file_name':vul_file_name}
                sources = sources.append(d_dict, ignore_index=True)

                # 每20000保存一次source，然后清空source
                if len(sources)%20000==0:
                    # 保存临时文件
                    temp_path = os.path.join(self.root,'temp_data',os.path.splitext(slice_graph)[0]+'_%s_.pkl'%store_pkl_count)
                    sources.to_pickle(temp_path)
                    print(temp_path)
                    sources = pd.DataFrame(columns=columns)
                    store_pkl_count+=1
            #（或者一个slice文件已经处理完毕）
            if len(sources)>0:
                # 保存临时文件
                temp_path = os.path.join(self.root,'temp_data', os.path.splitext(slice_graph)[0] + '_%s_.pkl' % store_pkl_count)
                sources.to_pickle(temp_path)
                print(temp_path)
                sources = pd.DataFrame(columns=columns)

    def update_tree_graph(self,tree_graph):
        '''
        更新tree_graph字段的值
        :param tree_graph:
        :return:
        '''
        # 将ast node的字符串转化为token 序列
        ast_node_tokens = tree_graph['ast_node_tokens']
        ast_node_token_lst = []
        for node_tokens in ast_node_tokens:
            node_token_lst = nltk.word_tokenize(node_tokens)
            ast_node_token_lst.append(node_token_lst)
        tree_graph['ast_node_token_lst'] = ast_node_token_lst

        # 将pdg node的字符串转化为token 序列
        pdg_node_tokens = tree_graph['pdg_node_tokens']
        instatements = []
        for node_tokens in pdg_node_tokens:
            if not node_tokens.endswith(';'):
                node_tokens = node_tokens + ';'
            node_token_lst = create_tokens(node_tokens)
            instatements.append(node_token_lst)
        tree_graph['pdg_node_token_lst'], _ = mapping(instatements)
        return tree_graph

    def merge_chunk_pkl(self):
        '''
        合并所有的数据快
        :param output_file:
        :return:
        '''
        columns = ['Testid', 'tree-graph', 'label', 'file_name']
        sources = pd.DataFrame(columns=columns)
        root = os.path.join(self.root, 'temp_data')
        for file in os.listdir(root):
            subset_data = pd.read_pickle(os.path.join(root, file))
            sources = pd.concat([sources, subset_data])
        return sources

    def merge_origin_features(self, input):
        graph_features = input['pdg_node_features'] + input['ast_node_features']
        return ' '.join(graph_features)

    def removeDupInGData(self, data):
        # remove duplicate instances in graph-based dataset
        print('The total number of instances in original data is %s' % len(data))
        data['feature_str'] = data.apply(lambda x: self.merge_origin_features(x['tree-graph']), axis=1)
        data.drop_duplicates('feature_str', keep='first', inplace=True)
        data.drop(columns=['feature_str'])
        print('The total number of instances in duplicated code is %s' % len(data))
        return data


    # split data for training, developing and testing
    def split_data(self, dataset_name):
        data_path = self.root+ '/'
        data = pd.read_pickle(data_path+dataset_name)
        data_num = len(data)
        ratios = [int(r) for r in self.ratio.split(':')]
        train_split = int(ratios[0] / sum(ratios) * data_num)
        val_split = train_split + int(ratios[1] / sum(ratios) * data_num)

        data = data.sample(frac=1, random_state=666)  # random_state=666
        train = data.iloc[:train_split]
        dev = data.iloc[train_split:val_split]
        test = data.iloc[val_split:]

        def check_or_create(path):
            if not os.path.exists(path):
                os.mkdir(path)

        train_path = data_path + 'train/'
        check_or_create(train_path)
        self.train_file_path = train_path + 'train_.pkl'
        train.to_pickle(self.train_file_path)

        dev_path = data_path + 'dev/'
        check_or_create(dev_path)
        self.dev_file_path = dev_path + 'dev_.pkl'
        dev.to_pickle(self.dev_file_path)

        test_path = data_path + 'test/'
        check_or_create(test_path)
        self.test_file_path = test_path + 'test_.pkl'
        test.to_pickle(self.test_file_path)

    # construct dictionary and train word embedding
    def dictionary_and_embedding(self, input_file, size):
        self.size = size
        if not input_file:
            input_file = os.path.join(self.root, 'train', 'train_.pkl')
        data = pd.read_pickle(input_file)
        embedding_path = os.path.join(self.root, 'embedding')
        if not os.path.exists(embedding_path):
            os.mkdir(embedding_path)

        def trans_to_sequences(ast_info):
            temp = []

            for x, y in zip(ast_info['pdg_node_features'], ast_info['pdg_node_token_lst']):
                temp.extend([x] + create_tokens(y))
            for x,y in zip(ast_info['ast_node_features'],ast_info['ast_node_token_lst']):
                if x ==' '.join(y):
                    temp.extend(y)
                else:
                    temp.extend([x]+y)
            return temp

        corpus = data['tree-graph'].apply(trans_to_sequences)
        # for index in range(len(trees)):
        #     print(index)
        #     print(trans_to_sequences(trees['code'].iloc[index]))

        str_corpus = [' '.join(c) for c in corpus]
        data['code'] = pd.Series(str_corpus)
        data.to_csv(embedding_path + '/programs_ns.tsv')

        from gensim.models.word2vec import Word2Vec
        w2v = Word2Vec(corpus, size=size, workers=16, sg=1, min_count=3)
        w2v.save(embedding_path + '/node_w2v_' + str(size))

    # generate block sequences with index representations
    def generate_block_seqs(self, dataset_name, size):
        from gensim.models.word2vec import Word2Vec

        path = pd.read_pickle(self.root + '/' + dataset_name + '/' + dataset_name + '_.pkl')
        embedding_path = os.path.join(self.root, 'embedding')
        word2vec = Word2Vec.load(embedding_path + '/node_w2v_' + str(size)).wv
        vocab = word2vec.vocab
        max_token = word2vec.syn0.shape[0]


        def trans_token_to_blocks(input):
            graph_features = []
            # 'pdg_node_features' 代表pdg node的feature （在前），'ast_node_features'代表pdg每个node对应的语法树里节点的feature（在后）
            # (1) graph_features的前top-k个节点是pdg节点（k是pdg节点的个数）
            for x, y in zip(input['pdg_node_features'], input['pdg_node_token_lst']):
                node_token_index = [vocab[token].index if token in vocab else max_token for token in [x]+create_tokens(y)]
                graph_features.append(node_token_index)

            # (2) graph_features top-k之后是ast节点
            for x,y in zip(input['ast_node_features'],input['ast_node_token_lst']):
                if x ==' '.join(y):
                    node_token_lst = y
                else:
                    node_token_lst = [x]+y
                node_token_index = [vocab[token].index if token in vocab else max_token for token in node_token_lst]
                graph_features.append(node_token_index)
            return graph_features

        def trans_edge_label(input):
            graph_adjacency_list = input['graph_adjacency_list']
            return graph_adjacency_list


        data = pd.DataFrame(path, copy=True)
        data['graph_features'] = data.apply(lambda x: trans_token_to_blocks(x['tree-graph']), axis=1)
        data['graph_adjacency'] = data.apply(lambda x: trans_edge_label(x['tree-graph']), axis=1)
        # if 'label' in data.columns:
        #     data.drop('label', axis=1, inplace=True)
        self.blocks = data
        data.to_pickle(os.path.join(self.root, dataset_name) + '/blocks.pkl')

    # run for processing data to train
    def run(self):
        print('parse source code...')
        dataset_input = 'All_data_input.pkl'
        # 1 生成数据
        # self.gen_data(input_slice_dir='../resources/cwe_labelled_slice_graphs')
        # # 2 预处理数据
        # data = self.merge_chunk_pkl()
        # data['tree-graph'] = data.apply(lambda x: self.update_tree_graph(x['tree-graph']), axis=1)
        # store_path = os.path.join(self.root, dataset_input)
        # data.to_pickle(store_path)

        print('split data...')
        self.split_data(dataset_input)
        print('train word embedding...')
        size = 128
        self.dictionary_and_embedding(None, size)
        print('generate block sequences...')
        self.generate_block_seqs('train', size)
        self.generate_block_seqs('dev', size)
        self.generate_block_seqs('test', size)

if __name__ == '__main__':
    pipline = DataPipline('3:1:1', 'cwe_graph_data')
    pipline.run()